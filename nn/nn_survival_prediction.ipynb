{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = pd.read_csv('static-pre-process.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "static_1 = static.copy()\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "static_1['race_encoded'] = scaler.fit_transform(static_1[['race_encoded']])\n",
    "static_1['icu_outtime'] = pd.to_datetime(static_1['icu_outtime'])\n",
    "static_1['icu_intime'] = pd.to_datetime(static_1['icu_intime'])\n",
    "\n",
    "# Calculate ICU stay duration in hours and keep only the total number of hours\n",
    "static_1['icu_hours'] = (static_1['icu_outtime'] - static_1['icu_intime']).dt.total_seconds() / 3600\n",
    "static_1.drop(['admission_type','first_careunit'], axis=1, inplace=True)\n",
    "static_1.sort_values(by='id', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20414, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_2 = static_1.loc[:,'admission_age':'gender_encoded']\n",
    "numpy_array = static_2.to_numpy()\n",
    "static_data = torch.tensor(numpy_array, dtype=torch.float32)\n",
    "static_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv('dynamic-pre-process.csv')\n",
    "ts1 = ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = ts1.groupby('id')\n",
    "tensor_list = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    values = group.loc[:,'albumin':].values\n",
    "    tensor_list.append(torch.tensor(values, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_ts = [t.shape[0] for t in tensor_list]\n",
    "average_length = sum(lst_ts) / len(lst_ts)\n",
    "math.ceil(average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_length = 5\n",
    "num_features = len(ts1.columns) - list(ts1.columns).index('albumin')\n",
    "final_tensors = []\n",
    "for t in tensor_list:\n",
    "    if t.shape[0] > ave_length:\n",
    "        t = t[:ave_length, :]\n",
    "    elif t.shape[0] < ave_length:\n",
    "        padding_needed = ave_length - t.shape[0]\n",
    "        padding = torch.zeros(padding_needed, num_features, dtype=t.dtype)\n",
    "        t = torch.cat([t, padding], dim=0)\n",
    "    final_tensors.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20414, 5, 68])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data = torch.stack(final_tensors)\n",
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20414, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_1 = static_1['icu_death']\n",
    "labels = torch.tensor(label_1.values.reshape(-1, 1), dtype=torch.float32)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('cleaned_notes.csv')\n",
    "text = text[['id','text']]\n",
    "text['text'] = text['text'].str.replace('\\n', ' ', regex=False)\n",
    "text.sort_values(by='id', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_merged = text.groupby('id')['text'].agg(' '.join).reset_index()\n",
    "text_data = text_merged['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, static_data, ts_data, texts, labels):\n",
    "        self.static_data = static_data\n",
    "        self.ts_data = ts_data\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_tokens = self.tokenizer(self.texts[idx], return_tensors='pt', padding=True, truncation=True, max_length=128, add_special_tokens=True)\n",
    "        return (\n",
    "            self.static_data[idx],\n",
    "            self.ts_data[idx],\n",
    "            text_tokens['input_ids'].squeeze(0),\n",
    "            text_tokens['attention_mask'].squeeze(0),\n",
    "            self.labels[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    static_data, ts_data, input_ids, attention_masks, labels = zip(*batch)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    static_data = torch.stack(static_data)\n",
    "    ts_data = torch.stack(ts_data)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return static_data, ts_data, input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "(train_static, test_static, \n",
    " train_ts, test_ts, \n",
    " train_texts, test_texts, \n",
    " train_labels, test_labels) = train_test_split(static_data, ts_data, text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_static, train_ts, train_texts, train_labels)\n",
    "test_dataset = CustomDataset(test_static, test_ts, test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        self.static_layer = nn.Sequential(\n",
    "            nn.Linear(16, 32), # 16 features\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=68, hidden_size=32, batch_first=True) #68 features\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 + 32 + 768, 64),  # LSTM and static 32 dim，BERT output 768 dim\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, static_data, ts_data, input_ids, attention_mask):\n",
    "        static_features = self.static_layer(static_data)\n",
    "        _, (hidden, _) = self.lstm(ts_data)\n",
    "        lstm_features = hidden[-1]\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        combined_features = torch.cat([static_features, lstm_features, text_features], dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.30614933371543884 (8, 0.0001)\n",
      "Epoch 2, Loss: 0.5118962526321411 (8, 0.0001)\n",
      "Epoch 3, Loss: 0.3002335727214813 (8, 0.0001)\n",
      "Epoch 4, Loss: 0.6778562068939209 (8, 0.0001)\n",
      "Epoch 5, Loss: 0.3189200460910797 (8, 0.0001)\n",
      "Epoch 6, Loss: 0.2772770822048187 (8, 0.0001)\n",
      "Epoch 7, Loss: 0.10982435941696167 (8, 0.0001)\n",
      "Epoch 8, Loss: 0.2964090406894684 (8, 0.0001)\n",
      "Epoch 9, Loss: 0.1279366910457611 (8, 0.0001)\n",
      "Epoch 10, Loss: 0.10461699217557907 (8, 0.0001)\n",
      "Test Accuracy: 0.8956649899482727 (8, 0.0001)\n",
      "Test AUC: 0.6642409373752312 (8, 0.0001)\n",
      "Epoch 1, Loss: 0.09966601431369781 (16, 0.0001)\n",
      "Epoch 2, Loss: 0.3088759481906891 (16, 0.0001)\n",
      "Epoch 3, Loss: 0.6870419979095459 (16, 0.0001)\n",
      "Epoch 4, Loss: 0.2912318706512451 (16, 0.0001)\n",
      "Epoch 5, Loss: 0.2893545925617218 (16, 0.0001)\n",
      "Epoch 6, Loss: 0.529589056968689 (16, 0.0001)\n",
      "Epoch 7, Loss: 0.3206197917461395 (16, 0.0001)\n",
      "Epoch 8, Loss: 0.14706763625144958 (16, 0.0001)\n",
      "Epoch 9, Loss: 0.08707202970981598 (16, 0.0001)\n",
      "Epoch 10, Loss: 0.5257540941238403 (16, 0.0001)\n",
      "Test Accuracy: 0.8956649899482727 (16, 0.0001)\n",
      "Test AUC: 0.6938349631101715 (16, 0.0001)\n",
      "Epoch 1, Loss: 0.12714973092079163 (32, 0.0001)\n",
      "Epoch 2, Loss: 0.0986618623137474 (32, 0.0001)\n",
      "Epoch 3, Loss: 0.3689378499984741 (32, 0.0001)\n",
      "Epoch 4, Loss: 0.27678900957107544 (32, 0.0001)\n",
      "Epoch 5, Loss: 0.6909568309783936 (32, 0.0001)\n",
      "Epoch 6, Loss: 0.1104164719581604 (32, 0.0001)\n",
      "Epoch 7, Loss: 0.41418224573135376 (32, 0.0001)\n",
      "Epoch 8, Loss: 0.18184995651245117 (32, 0.0001)\n",
      "Epoch 9, Loss: 0.08091793209314346 (32, 0.0001)\n",
      "Epoch 10, Loss: 0.22709409892559052 (32, 0.0001)\n",
      "Test Accuracy: 0.896644651889801 (32, 0.0001)\n",
      "Test AUC: 0.7008829937055566 (32, 0.0001)\n",
      "Epoch 1, Loss: 0.04109010472893715 (8, 0.001)\n",
      "Epoch 2, Loss: 0.07812394946813583 (8, 0.001)\n",
      "Epoch 3, Loss: 0.68265300989151 (8, 0.001)\n",
      "Epoch 4, Loss: 0.1628303974866867 (8, 0.001)\n",
      "Epoch 5, Loss: 1.1176339387893677 (8, 0.001)\n",
      "Epoch 6, Loss: 0.0841372087597847 (8, 0.001)\n",
      "Epoch 7, Loss: 0.38599613308906555 (8, 0.001)\n",
      "Epoch 8, Loss: 0.7999076843261719 (8, 0.001)\n",
      "Epoch 9, Loss: 0.05575529858469963 (8, 0.001)\n",
      "Epoch 10, Loss: 0.07340840250253677 (8, 0.001)\n",
      "Test Accuracy: 0.8956649899482727 (8, 0.001)\n",
      "Test AUC: 0.718250162720925 (8, 0.001)\n",
      "Epoch 1, Loss: 0.2644127607345581 (16, 0.001)\n",
      "Epoch 2, Loss: 0.09862029552459717 (16, 0.001)\n",
      "Epoch 3, Loss: 0.4201904833316803 (16, 0.001)\n",
      "Epoch 4, Loss: 0.33576473593711853 (16, 0.001)\n",
      "Epoch 5, Loss: 0.15833523869514465 (16, 0.001)\n",
      "Epoch 6, Loss: 0.3649251461029053 (16, 0.001)\n",
      "Epoch 7, Loss: 0.20666569471359253 (16, 0.001)\n",
      "Epoch 8, Loss: 0.17246535420417786 (16, 0.001)\n",
      "Epoch 9, Loss: 0.3440650999546051 (16, 0.001)\n",
      "Epoch 10, Loss: 0.4202393889427185 (16, 0.001)\n",
      "Test Accuracy: 0.8956649899482727 (16, 0.001)\n",
      "Test AUC: 0.7189684456204 (16, 0.001)\n",
      "Epoch 1, Loss: 0.4040044844150543 (32, 0.001)\n",
      "Epoch 2, Loss: 0.08453289419412613 (32, 0.001)\n",
      "Epoch 3, Loss: 0.059937845915555954 (32, 0.001)\n",
      "Epoch 4, Loss: 0.2827320396900177 (32, 0.001)\n",
      "Epoch 5, Loss: 0.07905668020248413 (32, 0.001)\n",
      "Epoch 6, Loss: 0.4046289920806885 (32, 0.001)\n",
      "Epoch 7, Loss: 0.1137271597981453 (32, 0.001)\n",
      "Epoch 8, Loss: 0.0455169677734375 (32, 0.001)\n",
      "Epoch 9, Loss: 0.0830422043800354 (32, 0.001)\n",
      "Epoch 10, Loss: 0.34957557916641235 (32, 0.001)\n",
      "Test Accuracy: 0.8907666206359863 (32, 0.001)\n",
      "Test AUC: 0.7284361074843922 (32, 0.001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for i in [(8,0.0001),(16,0.0001),(32,0.0001),(8,0.001),(16,0.001),(32,0.001)]:\n",
    "    class MultiInputModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MultiInputModel, self).__init__()\n",
    "            self.static_layer = nn.Sequential(\n",
    "                nn.Linear(16, i[0]), # 16 features\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.lstm = nn.LSTM(input_size=68, hidden_size=i[0], batch_first=True) #68 features\n",
    "            self.bert = bert_model\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(i[0] + i[0] + 768, i[0]),  # LSTM and static 32 dim，BERT output 768 dim\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(i[0], 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        def forward(self, static_data, ts_data, input_ids, attention_mask):\n",
    "            static_features = self.static_layer(static_data)\n",
    "            _, (hidden, _) = self.lstm(ts_data)\n",
    "            lstm_features = hidden[-1]\n",
    "            text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "            combined_features = torch.cat([static_features, lstm_features, text_features], dim=1)\n",
    "            output = self.fc(combined_features)\n",
    "            return output\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model = MultiInputModel()\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=i[1])\n",
    "    num_epochs = 10\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            static_data, ts_data, input_ids, attention_mask, labels = data\n",
    "\n",
    "            static_data = static_data.to(device)\n",
    "            ts_data = ts_data.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float() \n",
    "\n",
    "            output = model(static_data, ts_data, input_ids, attention_mask)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = loss_function(output, labels.float())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}',i)\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            static_data, ts_data, input_ids, attention_mask, labels = data\n",
    "\n",
    "            static_data = static_data.to(device)\n",
    "            ts_data = ts_data.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            output = model(static_data, ts_data, input_ids, attention_mask)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            predictions = (output > 0.5).float()\n",
    "            correct = (predictions == labels).float().sum()\n",
    "\n",
    "            total_correct += correct\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / num_samples\n",
    "    print(f'Test Accuracy: {accuracy.item()}',i)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            static_data, ts_data, input_ids, attention_mask, labels = data\n",
    "\n",
    "            static_data = static_data.to(device)\n",
    "            ts_data = ts_data.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            output = model(static_data, ts_data, input_ids, attention_mask)\n",
    "\n",
    "            all_predictions.extend(output.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    auc = roc_auc_score(all_labels, all_predictions)\n",
    "    print(f'Test AUC: {auc}',i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8802351355552673\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "num_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        static_data, ts_data, input_ids, attention_mask, labels = data\n",
    "\n",
    "        static_data = static_data.to(device)\n",
    "        ts_data = ts_data.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        output = model(static_data, ts_data, input_ids, attention_mask)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        predictions = (output > 0.5).float()\n",
    "        correct = (predictions == labels).float().sum()\n",
    "\n",
    "        total_correct += correct\n",
    "        num_samples += labels.size(0)\n",
    "\n",
    "accuracy = total_correct / num_samples\n",
    "print(f'Test Accuracy: {accuracy.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        static_data, ts_data, input_ids, attention_mask, labels = data\n",
    "\n",
    "        static_data = static_data.to(device)\n",
    "        ts_data = ts_data.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        output = model(static_data, ts_data, input_ids, attention_mask)\n",
    "\n",
    "        all_predictions.extend(output.view(-1).cpu().numpy())\n",
    "        all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "print(f'Test AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
